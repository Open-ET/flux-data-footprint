{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script was developed to parallel process preformatted time series of input data needed for\n",
    "the Kljun et. al 2d flux footprint prediction code and ultimately create monthly-ETo-weighted\n",
    "footprint georeferenced footprint rasters. \n",
    "\n",
    "Checks are performed on the input data to handle data quality issues. The weighting method \n",
    "uses normalized hourly proportions of ASCE ETo computed from NLDAS v2 data for the closest cell.\n",
    "NLDAS data is automatically downloaded using OpenDAP given Earthdata login info. Only months\n",
    "with 20 days worth of good hourly data are used, specifically the criteria 13*20 = 260 or more hours\n",
    "of data (only from hours between 6:00AM to 8:00 PM) must exist in a month. Checks are performed\n",
    "to ensure final weighting procedure was successful at different steps of the process.\n",
    "\n",
    "This script is not intended to be used by others but to document a workflow that was employed for\n",
    "scientific purposes.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import calc_footprint_FFP_climatology as ffp\n",
    "import footprint_funcs as ff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import refet\n",
    "import pyproj as proj\n",
    "import xarray\n",
    "import requests\n",
    "import multiprocessing as mp\n",
    "__author__='John Volk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata that has each sites' elevation used in ETr/ETo calcs\n",
    "AMF_meta_path = Path('path/to/site_metadata')\n",
    "AMF_meta = pd.read_csv(AMF_meta_path, index_col='SITE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify path with input CSV files for each station with \n",
    "# input time series of needed data, e.g. zm, u_star, L,...\n",
    "in_dir = Path('dir/with/input')\n",
    "hourly_files = list(in_dir.glob('*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_compiled_input(path):\n",
    "    \"\"\"\n",
    "    Check if required input data exists in file and is formatted appropriately.\n",
    "    \n",
    "    Input files should be hourly or finer temporal frequency, drops hours\n",
    "    without required input data. \n",
    "    \"\"\"\n",
    "    need_vars = {'latitude','longitude','ET_corr','wind_dir','u_star','sigma_v','zm','hc','d','L'}\n",
    "    #don't parse dates first check if required inputs exist to save processing time\n",
    "    df=pd.read_csv(path, index_col='date', parse_dates=False)\n",
    "    cols = df.columns\n",
    "    check_1 = need_vars.issubset(cols)\n",
    "    check_2 = len({'u_mean','z0'}.intersection(cols)) >= 1 # need one or the other\n",
    "    # if either test failed then insufficient input data for footprint, abort\n",
    "    if not check_1 or not check_2:\n",
    "        return None\n",
    "    ret = df\n",
    "    ret.index = pd.to_datetime(df.index)\n",
    "    ret = ret.resample('H').mean()    \n",
    "    lat,lon = ret[['latitude','longitude']].values[0]\n",
    "    keep_vars = need_vars.union({'u_mean','z0','IGBP_land_classification','secondary_veg_type'})\n",
    "    drop_vars = list(set(cols).difference(keep_vars))\n",
    "    ret.drop(drop_vars, 1, inplace=True)\n",
    "    ret.dropna(subset=['wind_dir','u_star','sigma_v','d','zm','L','ET_corr'], how='any', inplace=True)\n",
    "    return (ret, lat, lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(path, ed_user, ed_pass):\n",
    "    \"\"\"\n",
    "    Given path to time series of site hourly (or finer) input data,\n",
    "    compute daily ETo weighted footprint rasters. \n",
    "    \n",
    "    Requires NASA Earthdata username and password to download NLDAS-v2\n",
    "    primary forcing at point locations for estimated ASCE short ref. ET.\n",
    "    \"\"\"\n",
    "    station = path.stem\n",
    "    res = read_compiled_input(path)\n",
    "    if res is None: \n",
    "        print(f'Insufficient data exists for site: {station} skipping.')\n",
    "        return\n",
    "    \n",
    "    df, latitude, longitude = res\n",
    "    elevation = AMF_meta.loc[station, 'station_elevation']\n",
    "\n",
    "    station_coord = (longitude, latitude)\n",
    "    # get EPSG code from lat,long, convert to UTM\n",
    "    EPSG=32700-np.round((45+latitude)/90.0)*100+np.round((183+longitude)/6.0)\n",
    "    EPSG = int(EPSG)\n",
    "    in_proj = proj.Proj(init='EPSG:4326')\n",
    "    out_proj = proj.Proj(init='EPSG:{}'.format(EPSG))\n",
    "    (station_x,station_y) = proj.transform(in_proj,out_proj,*station_coord)\n",
    "    print('original coordinates:',station_x,station_y)\n",
    "    # move coord to snap centroid to 30m grid, minimal distortion\n",
    "    rx = station_x % 15\n",
    "    if rx > 7.5:\n",
    "        station_x += (15-rx)\n",
    "        # final coords should be odd factors of 15\n",
    "        if (station_x / 15) % 2 == 0:\n",
    "            station_x -= 15\n",
    "    else:    \n",
    "        station_x -= rx\n",
    "        if (station_x / 15) % 2 == 0:\n",
    "            station_x += 15\n",
    "    ry = station_y % 15\n",
    "    if ry > 7.5:\n",
    "        print('ry > 7.5')\n",
    "        station_y += (15-ry )\n",
    "        if (station_y / 15) % 2 == 0:\n",
    "            station_y -= 15\n",
    "    else:\n",
    "        print('ry <= 7.5')\n",
    "        station_y -= ry\n",
    "        if (station_y / 15) % 2 == 0:\n",
    "            station_y += 15\n",
    "    print('adjusted coordinates:',station_x,station_y)\n",
    "\n",
    "    #Other model parameters\n",
    "    h_s = 2000. #Height of atmos. boundary layer [m] - assumed\n",
    "    dx = 30. #Model resolution [m]\n",
    "    origin_d = 300. #Model bounds distance from origin [m]\n",
    "    #modify if needed\n",
    "    start_hr = 6 # hours from 1 to 24\n",
    "    end_hr = 18\n",
    "    hours_zero_indexed = np.arange(start_hr-1,end_hr)\n",
    "    hours_one_indexed = np.arange(start_hr,end_hr+1)\n",
    "\n",
    "    n_hrs = len(hours_zero_indexed) \n",
    "\n",
    "    nldas_out_dir = Path('NLDAS_data')\n",
    "    if not nldas_out_dir.is_dir():\n",
    "        nldas_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_dir = Path('All_output')/'AMF_monthly'/f'{station}'\n",
    "\n",
    "    if not out_dir.is_dir():\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df['date'] = df.index\n",
    "    need_hrs = n_hrs * 20 # 20 days worth of hours (6-8)\n",
    "    months = [g for n, g in df.groupby(pd.Grouper(key='date',freq='M'))]\n",
    "\n",
    "    #Loop through monthly grouped slices\n",
    "    for mdf in months:\n",
    "        #Subset dataframe to only values in day of year\n",
    "        year_str = mdf.index.year[0]\n",
    "        month_str = mdf.index.month[0]\n",
    "        print(f'Date: {month_str}/{year_str}')\n",
    "        \n",
    "        temp_df=mdf.between_time(f'{start_hr:02}:00', f'{end_hr:02}:00')\n",
    "        actual_hrs = len(temp_df)\n",
    "\n",
    "        # check on n hours per day\n",
    "        if actual_hrs < need_hrs:\n",
    "            print(f'Less than {need_hrs} hours of data for {month_str}/{year_str}, skipping.')\n",
    "            continue\n",
    "\n",
    "        new_dat = None\n",
    "\n",
    "        out_f = out_dir/ f'{year_str}-{month_str:02}.tif'\n",
    "\n",
    "        final_outf = out_dir/f'{year_str}-{month_str:02}_weighted.tif'\n",
    "        if final_outf.is_file():\n",
    "            print(f'final monthly weighted footprint already wrote to: {final_outf}\\nskipping.')\n",
    "            continue # do not overwrite date/site raster \n",
    "\n",
    "        # make hourly band raster for the day\n",
    "        band=1\n",
    "        for date, temp_line in temp_df.iterrows():\n",
    "            hour = date.hour\n",
    "            print(f'Date: {year_str}/{month_str}/{date.day}, Hour: {hour}, Band: {band}')\n",
    "\n",
    "            try:\n",
    "                if temp_line.empty: \n",
    "                    print(f'Missing all data for {date,hour} skipping')\n",
    "                    continue\n",
    "\n",
    "                zm = temp_line.zm - temp_line.d\n",
    "                z0 = np.array(temp_line.z0) if 'z0' in temp_line else None\n",
    "                u_mean = np.array(temp_line.u_mean) if 'u_mean' in temp_line else None\n",
    "                if u_mean is not None: z0 = None\n",
    "                \n",
    "                #Calculate footprint\n",
    "                temp_ffp = ffp.FFP_climatology(domain=[-origin_d,origin_d,-origin_d,origin_d],dx=dx,dy=dx,\n",
    "                                        zm=np.array(zm), h=np.array(h_s), rs=None, \n",
    "                                        z0=z0, ol=np.array(temp_line['L']),\n",
    "                                        sigmav=np.array(temp_line['sigma_v']),\n",
    "                                        ustar=np.array(temp_line['u_star']), \n",
    "                                        umean=u_mean,\n",
    "                                        wind_dir=np.array(temp_line['wind_dir']),\n",
    "                                        crop=0,fig=0,verbosity=0)\n",
    "                ####verbosoity=2 prints out errors; if z0 triggers errors, use umean\n",
    "\n",
    "                f_2d = np.array(temp_ffp['fclim_2d'])\n",
    "                x_2d = np.array(temp_ffp['x_2d']) + station_x\n",
    "                y_2d = np.array(temp_ffp['y_2d']) + station_y\n",
    "                f_2d = f_2d*dx**2\n",
    "                \n",
    "                #Calculate affine transform for given x_2d and y_2d\n",
    "                affine_transform = ff.find_transform(x_2d,y_2d)\n",
    "\n",
    "                #Create data file if not already created\n",
    "                if new_dat is None:\n",
    "                    #print(f_2d.shape)\n",
    "                    new_dat = rasterio.open(\n",
    "                        out_f,'w',driver='GTiff',dtype=rasterio.float64,\n",
    "                        count=actual_hrs,height=f_2d.shape[0],width=f_2d.shape[1],\n",
    "                        transform=affine_transform, crs=out_proj.srs,\n",
    "                        nodata=0.00000000e+000\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "\n",
    "                print(f'Hour {hour} footprint failed, band {band} not written.')\n",
    "\n",
    "                temp_ffp = None\n",
    "                band+=1\n",
    "                continue\n",
    "\n",
    "            #Mask out points that are below a % threshold (defaults to 90%)\n",
    "            f_2d = ff.mask_fp_cutoff(f_2d)\n",
    "\n",
    "            #Write the new band\n",
    "            new_dat.write(f_2d, band)\n",
    "\n",
    "            #Update tags with metadata\n",
    "            tag_dict = {'hour':f'{hour*100:04}',\n",
    "                        'wind_dir':np.array(temp_line['wind_dir']),\n",
    "                        'total_footprint':np.nansum(f_2d)}\n",
    "\n",
    "            new_dat.update_tags(band,**tag_dict)\n",
    "            \n",
    "            band+=1\n",
    "\n",
    "        #Close dataset if it exists\n",
    "        try:\n",
    "            new_dat.close()\n",
    "        except:\n",
    "            print(f'ERROR: could not write footprint for site: {station}:\\nto: {out_f}')\n",
    "            continue # skip to next month...\n",
    "            \n",
    "        \n",
    "    \n",
    "        # for NLDAS data from pymetric\n",
    "        for date, temp_line in temp_df.iterrows():\n",
    "            hour = date.hour\n",
    "            #NLDAS version 2, primary forcing set (a), DOY must be 3 digit zero padded, HH 2-digit between 00-23, MM and DD also 2 digit\n",
    "            YYYY = date.year\n",
    "            DOY = date.timetuple().tm_yday\n",
    "            MM = date.month\n",
    "            DD = date.day\n",
    "            HH = hour\n",
    "\n",
    "            nldas_outf_path = nldas_out_dir / f'{YYYY}_{MM:02}_{DD:02}_{HH:02}.grb'\n",
    "            if nldas_outf_path.is_file():\n",
    "                print(f'{nldas_outf_path} already exists, not overwriting.')\n",
    "                pass\n",
    "                # do not overwrite!\n",
    "            else:\n",
    "                data_url = f'https://hydro1.gesdisc.eosdis.nasa.gov/data/NLDAS/NLDAS_FORA0125_H.002/{YYYY}/{DOY:03}/NLDAS_FORA0125_H.A{YYYY}{MM:02}{DD:02}.{HH:02}00.002.grb'\n",
    "                session = requests.Session()\n",
    "                r1 = session.request('get', data_url)\n",
    "                r = session.get(r1.url, stream=True, auth=(ed_user, ed_pass))\n",
    "\n",
    "                # write grib file temporarily\n",
    "                with open(nldas_outf_path, 'wb') as outf:\n",
    "                    for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                        if chunk:  # filter out keep-alive new chunks\n",
    "                            outf.write(chunk)\n",
    "\n",
    "\n",
    "        # get hourly time series of ETr and ETo, save\n",
    "        for date, temp_line in temp_df.iterrows():\n",
    "            hour = date.hour\n",
    "            #NLDAS version 2, primary forcing set (a), DOY must be 3 digit zero padded, HH 2-digit between 00-23, MM and DD also 2 digit\n",
    "            YYYY = date.year\n",
    "            DOY = date.timetuple().tm_yday\n",
    "            MM = date.month\n",
    "            DD = date.day\n",
    "            HH = hour\n",
    "            # already ensured to exist above loop\n",
    "            nldas_outf_path = nldas_out_dir / f'{YYYY}_{MM:02}_{DD:02}_{HH:02}.grb'\n",
    "\n",
    "            # open grib and extract needed data at nearest gridcell, calc ETr/ETo anf append to time series\n",
    "            ds = xarray.open_dataset(nldas_outf_path,engine='pynio').sel(lat_110=latitude, lon_110=longitude, method='nearest')\n",
    "            # calculate hourly ea from specific humidity\n",
    "            pair = ds.get('PRES_110_SFC').data / 1000 # nldas air pres in Pa convert to kPa\n",
    "            sph = ds.get('SPF_H_110_HTGL').data # kg/kg\n",
    "            ea = refet.calcs._actual_vapor_pressure(q=sph, pair=pair) # ea in kPa\n",
    "            # calculate hourly wind\n",
    "            wind_u = ds.get('U_GRD_110_HTGL').data\n",
    "            wind_v = ds.get('V_GRD_110_HTGL').data\n",
    "            wind = np.sqrt(wind_u ** 2 + wind_v ** 2)\n",
    "            # get temp convert to C\n",
    "            temp = ds.get('TMP_110_HTGL').data - 273.15\n",
    "            # get rs\n",
    "            rs = ds.get('DSWRF_110_SFC').data\n",
    "            unit_dict = {'rs': 'w/m2'}\n",
    "            # create refet object for calculating\n",
    "\n",
    "            refet_obj = refet.Hourly(\n",
    "                tmean=temp, ea=ea, rs=rs, uz=wind,\n",
    "                zw=zm, elev=elevation, lat=latitude, lon=longitude,\n",
    "                doy=DOY, time=HH, method='asce', input_units=unit_dict) #HH must be int\n",
    "\n",
    "            # this one is saved under the site_ID subdir\n",
    "            nldas_ts_outf = out_dir/ f'nldas_ETr.csv'\n",
    "            # save/append time series of point data\n",
    "            dt = pd.datetime(YYYY,MM,DD,HH)\n",
    "            ETr_df = pd.DataFrame(columns=['ETr','ETo','ea','sph','wind','pair','temp','rs'])\n",
    "            ETr_df.loc[dt, 'ETr'] = refet_obj.etr()[0]\n",
    "            ETr_df.loc[dt, 'ETo'] = refet_obj.eto()[0]\n",
    "            ETr_df.loc[dt, 'ea'] = ea[0]\n",
    "            ETr_df.loc[dt, 'sph'] = sph\n",
    "            ETr_df.loc[dt, 'wind'] = wind\n",
    "            ETr_df.loc[dt, 'pair'] = pair\n",
    "            ETr_df.loc[dt, 'temp'] = temp\n",
    "            ETr_df.loc[dt, 'rs'] = rs\n",
    "            ETr_df.index.name = 'date'\n",
    "\n",
    "            # if first run save file with individual datetime (hour data) else open and overwrite hour\n",
    "            if not nldas_ts_outf.is_file():\n",
    "                ETr_df.round(4).to_csv(nldas_ts_outf)\n",
    "            else:\n",
    "                curr_df = pd.read_csv(nldas_ts_outf, index_col='date', parse_dates=True)\n",
    "                curr_df.loc[dt] = ETr_df.loc[dt]\n",
    "                curr_df.round(4).to_csv(nldas_ts_outf)    \n",
    "\n",
    "        # do hourly weighting - do not necessarily need to do this all in the same loop\n",
    "        src = rasterio.open(out_f)\n",
    "        # hourly fetch scalar sums\n",
    "        global_sum = np.zeros(shape=(actual_hrs))\n",
    "        for hour in range(1,actual_hrs+1):\n",
    "            arr = src.read(hour)\n",
    "            global_sum[hour-1] = arr.sum()\n",
    "        # normalized fetch rasters\n",
    "        normed_fetch_rasters = [] \n",
    "        for hour in range(1,actual_hrs+1):\n",
    "            arr = src.read(hour)\n",
    "            tmp = arr / global_sum[hour-1]\n",
    "            if np.isnan(tmp).all():\n",
    "                tmp = np.zeros_like(tmp)\n",
    "            normed_fetch_rasters.append(tmp)\n",
    "        # get NLDAS ts calc fraction of daily ETo\n",
    "        nldas_df = pd.read_csv(nldas_ts_outf, index_col='date', parse_dates=True).sort_index()\n",
    "        ETo = nldas_df.loc[\n",
    "            (nldas_df.index.year==year_str)&(nldas_df.index.month==month_str),'ETo'\n",
    "        ]\n",
    "        min_max_normed_ETo = (ETo-min(ETo))/(max(ETo)-min(ETo)) # deal with negative ETo value proportions\n",
    "        # take out hours where footprint does not exist\n",
    "        i = 0\n",
    "        for e, s in zip(min_max_normed_ETo.values, global_sum):\n",
    "            if s == 0:\n",
    "                min_max_normed_ETo.iloc[i] = 0\n",
    "            i+=1\n",
    "            \n",
    "        # after removing hours now calculate hourly proportions\n",
    "        nldas_df.loc[\n",
    "            (nldas_df.index.year == year_str) & (nldas_df.index.month == month_str), \n",
    "            'ETo_hr_props'\n",
    "        ] = min_max_normed_ETo / min_max_normed_ETo.sum()\n",
    "        \n",
    "\n",
    "        # weight normed hourly fetch rasters by hourly ETo proportions\n",
    "        month_slice = nldas_df.loc[\n",
    "            (nldas_df.index.year == year_str) & (nldas_df.index.month == month_str)\n",
    "        ]#.copy()\n",
    "        prop_col_indx=month_slice.columns.get_loc('ETo_hr_props')\n",
    "        for i in range(actual_hrs): # \n",
    "            normed_fetch_rasters[i] =\\\n",
    "                normed_fetch_rasters[i]*month_slice.iloc[i, prop_col_indx]\n",
    "        # save hourly proportions to time series file\n",
    "        nldas_df.round(4).to_csv(nldas_ts_outf)\n",
    "\n",
    "        # Last calculation, sum the weighted hourly rasters to a single monthly fetch raster\n",
    "        final_footprint = sum(normed_fetch_rasters)\n",
    "        assert np.isclose(final_footprint.sum(), 1), f'check 1 failed! {final_footprint.sum()}\\n'\n",
    "        # next check\n",
    "        for hour, raster in enumerate(normed_fetch_rasters):\n",
    "            assert np.isclose(\n",
    "                month_slice.iloc[hour, prop_col_indx], raster.sum()\n",
    "            ), f'check 2 failed for hour {hour}!!!'\n",
    "\n",
    "        # finally, write daily corrected raster with UTM zone reference \n",
    "        corr_raster_path = final_outf\n",
    "        out_raster = rasterio.open(\n",
    "            corr_raster_path,'w',driver='GTiff',dtype=rasterio.float64,\n",
    "            count=1,height=final_footprint.shape[0],width=final_footprint.shape[1],\n",
    "            transform=src.transform, crs=out_proj.srs, nodata=0.00000000e+000\n",
    "        )\n",
    "        out_raster.write(final_footprint,1)\n",
    "        out_raster.close()\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(processes=8)\n",
    "pool.map(runner,hourly_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
