{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T00:22:27.423706400Z",
     "start_time": "2024-03-30T00:22:22.158898600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paulinkenbrandt\\Anaconda3\\envs\\pygis311v3\\Lib\\site-packages\\pyproj\\__init__.py:89: UserWarning: pyproj unable to set database path.\n",
      "  _pyproj_global_context_initialize()\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script was developed to parallel process preformatted time series of input data needed for\n",
    "the Kljun et. al 2d flux footprint prediction code and ultimately create daily-ETo-weighted\n",
    "footprint georeferenced footprint rasters. \n",
    "\n",
    "Checks are performed on the input data to handle data quality issues. The weighting method \n",
    "uses normalized hourly proportions of ASCE ETo computed from NLDAS v2 data for the closest cell.\n",
    "NLDAS data is automatically downloaded using OpenDAP given Earthdata login info. Only days with \n",
    "5 or more hours of data (only from hours between 6:00AM to 8:00 PM) must exist in a day. \n",
    "Checks are performed to ensure final weighting procedure was successful at different steps of \n",
    "the process. \n",
    "\n",
    "This script is not intended to be used by others but to document a workflow that was employed for\n",
    "scientific purposes.\n",
    "\"\"\"\n",
    "from pathlib import Path\n",
    "import calc_footprint_FFP_climatology as ffp\n",
    "import footprint_funcs as ff\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import refet\n",
    "import pyproj as proj\n",
    "import xarray\n",
    "import requests\n",
    "import multiprocessing as mp\n",
    "__author__='John Volk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T00:22:28.187229700Z",
     "start_time": "2024-03-30T00:22:27.421618100Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path\\\\to\\\\site_metadata'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# read metadata that has each sites' elevation used in ETr/ETo calcs\u001B[39;00m\n\u001B[0;32m      2\u001B[0m AMF_meta_path \u001B[38;5;241m=\u001B[39m Path(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpath/to/site_metadata\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m AMF_meta \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mAMF_meta_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex_col\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mSITE_ID\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\pygis311v3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    936\u001B[0m     dialect,\n\u001B[0;32m    937\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    944\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m    945\u001B[0m )\n\u001B[0;32m    946\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 948\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\pygis311v3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    608\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    610\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 611\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    613\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    614\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\pygis311v3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1445\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1447\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1448\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\pygis311v3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1703\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1704\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1705\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1706\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1707\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1708\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1709\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1710\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1711\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1712\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1713\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1714\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1715\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1716\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\Anaconda3\\envs\\pygis311v3\\Lib\\site-packages\\pandas\\io\\common.py:863\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    858\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    860\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    861\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    862\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 863\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    864\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhandle\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    865\u001B[0m \u001B[43m            \u001B[49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    866\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mioargs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    867\u001B[0m \u001B[43m            \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    868\u001B[0m \u001B[43m            \u001B[49m\u001B[43mnewline\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    869\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    871\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    872\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'path\\\\to\\\\site_metadata'"
     ]
    }
   ],
   "source": [
    "# read metadata that has each sites' elevation used in ETr/ETo calcs\n",
    "AMF_meta_path = Path('path/to/site_metadata')\n",
    "AMF_meta = pd.read_csv(AMF_meta_path, index_col='SITE_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify path with input CSV files for each station with \n",
    "# input time series of needed data, e.g. zm, u_star, L,...\n",
    "in_dir = Path('dir/with/input')\n",
    "hourly_files = list(in_dir.glob('*.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_compiled_input(path):\n",
    "    \"\"\"\n",
    "    Check if required input data exists in file and is formatted appropriately.\n",
    "    \n",
    "    Input files should be hourly or finer temporal frequency, drops hours\n",
    "    without required input data. \n",
    "    \"\"\"\n",
    "    ret = None\n",
    "    need_vars = {'latitude','longitude','ET_corr','wind_dir','u_star','sigma_v','zm','hc','d','L'}\n",
    "    #don't parse dates first check if required inputs exist to save processing time\n",
    "    df=pd.read_csv(path, index_col='date', parse_dates=False)\n",
    "    cols = df.columns\n",
    "    check_1 = need_vars.issubset(cols)\n",
    "    check_2 = len({'u_mean','z0'}.intersection(cols)) >= 1 # need one or the other\n",
    "    # if either test failed then insufficient input data for footprint, abort\n",
    "    if not check_1 or not check_2:\n",
    "        return ret\n",
    "    ret = df\n",
    "    ret.index = pd.to_datetime(df.index)\n",
    "    ret = ret.resample('H').mean()\n",
    "    lat,lon = ret[['latitude','longitude']].values[0]\n",
    "    keep_vars = need_vars.union({'u_mean','z0','IGBP_land_classification','secondary_veg_type'})\n",
    "    drop_vars = list(set(cols).difference(keep_vars))\n",
    "    ret.drop(drop_vars, 1, inplace=True)\n",
    "    ret.dropna(subset=['wind_dir','u_star','sigma_v','d','zm','L','ET_corr'], how='any', inplace=True)\n",
    "    return ret, lat, lon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now have hourly input data including station coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runner(path, ed_user, ed_pass):\n",
    "    \"\"\"\n",
    "    Given path to time series of site hourly (or finer) input data,\n",
    "    compute daily ETo weighted footprint rasters. \n",
    "    \n",
    "    Requires NASA Earthdata username and password to download NLDAS-v2\n",
    "    primary forcing at point locations for estimated ASCE short ref. ET.\n",
    "    \"\"\"\n",
    "        \n",
    "    df, latitude, longitude = read_compiled_input(path)\n",
    "    station = path.stem\n",
    "    elevation = AMF_meta.loc[station, 'station_elevation']\n",
    "\n",
    "    station_coord = (longitude, latitude)\n",
    "    # get EPSG code from lat,long, convert to UTM\n",
    "    EPSG=32700-np.round((45+latitude)/90.0)*100+np.round((183+longitude)/6.0)\n",
    "    EPSG = int(EPSG)\n",
    "    in_proj = proj.Proj(init='EPSG:4326')\n",
    "    out_proj = proj.Proj(init='EPSG:{}'.format(EPSG))\n",
    "    (station_x,station_y) = proj.transform(in_proj,out_proj,*station_coord)\n",
    "    print('original coordinates:',station_x,station_y)\n",
    "    # move coord to snap centroid to 30m grid, minimal distortion\n",
    "    rx = station_x % 15\n",
    "    if rx > 7.5:\n",
    "        station_x += (15-rx)\n",
    "        # final coords should be odd factors of 15\n",
    "        if (station_x / 15) % 2 == 0:\n",
    "            station_x -= 15\n",
    "    else:    \n",
    "        station_x -= rx\n",
    "        if (station_x / 15) % 2 == 0:\n",
    "            station_x += 15\n",
    "            \n",
    "    ry = station_y % 15\n",
    "    if ry > 7.5:\n",
    "        print('ry > 7.5')\n",
    "        station_y += (15-ry )\n",
    "        if (station_y / 15) % 2 == 0:\n",
    "            station_y -= 15\n",
    "    else:\n",
    "        print('ry <= 7.5')\n",
    "        station_y -= ry\n",
    "        if (station_y / 15) % 2 == 0:\n",
    "            station_y += 15\n",
    "    print('adjusted coordinates:',station_x,station_y)\n",
    "\n",
    "    #Other model parameters\n",
    "    h_s = 2000. #Height of atmos. boundary layer [m] - assumed\n",
    "    dx = 30. #Model resolution [m]\n",
    "    origin_d = 300. #Model bounds distance from origin [m]\n",
    "    #modify if needed\n",
    "    start_hr = 6 # hours from 1 to 24\n",
    "    end_hr = 18\n",
    "    hours_zero_indexed = np.arange(start_hr-1,end_hr)\n",
    "    hours_one_indexed = np.arange(start_hr,end_hr+1)\n",
    "\n",
    "    n_hrs = len(hours_zero_indexed) \n",
    "\n",
    "    nldas_out_dir = Path('NLDAS_data')\n",
    "    if not nldas_out_dir.is_dir():\n",
    "        nldas_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_dir = Path('All_output')/'AMF'/f'{station}'\n",
    "\n",
    "    if not out_dir.is_dir():\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    #Loop through each day in the dataframe\n",
    "    for date in df.index.date:\n",
    "        #Subset dataframe to only values in day of year\n",
    "        print(f'Date: {date}')\n",
    "        temp_df = df[df.index.date == date]\n",
    "        temp_df=temp_df.between_time(f'{start_hr:02}:00', f'{end_hr:02}:00')\n",
    "        # check on n hours per day\n",
    "        if len(temp_df) < 5:\n",
    "            print(f'Less than 5 hours of data on {date}, skipping.')\n",
    "            continue\n",
    "            \n",
    "        new_dat = None\n",
    "\n",
    "        out_f = out_dir/ f'{date}.tif'\n",
    "\n",
    "        final_outf = out_dir/f'{date.year}-{date.month:02}-{date.day:02}_weighted.tif'\n",
    "        if final_outf.is_file():\n",
    "            print(f'final daily weighted footprint already wrote to: {final_outf}\\nskipping.')\n",
    "            continue # do not overwrite date/site raster \n",
    "\n",
    "        # make hourly band raster for the day\n",
    "        for indx, hour in enumerate(hours_zero_indexed):\n",
    "\n",
    "            band = indx + 1\n",
    "            print(f'Hour: {hour}')\n",
    "\n",
    "            try:\n",
    "                temp_line = temp_df.loc[temp_df.index.hour == hour,:]\n",
    "                if temp_line.empty: \n",
    "                    print(f'Missing all data for {date,hour} skipping')\n",
    "                    continue\n",
    "                zm = temp_line.zm.values - temp_line.d.values\n",
    "                z0 = temp_line.z0.values if 'z0' in temp_line.columns else None\n",
    "                u_mean = temp_line.u_mean.values if 'u_mean' in temp_line.columns else None\n",
    "                if u_mean is not None: z0 = None\n",
    "\n",
    "                #Calculate footprint\n",
    "                temp_ffp = ffp.FFP_climatology(domain=[-origin_d,origin_d,-origin_d,origin_d],dx=dx,dy=dx,\n",
    "                                        zm=zm, h=h_s, rs=None, z0=z0, \n",
    "                                        ol=temp_line['L'].values,sigmav=temp_line['sigma_v'].values,\n",
    "                                        ustar=temp_line['u_star'].values, umean=u_mean,\n",
    "                                        wind_dir=temp_line['wind_dir'].values,\n",
    "                                        crop=0,fig=0,verbosity=0)\n",
    "                f_2d = np.array(temp_ffp['fclim_2d'])\n",
    "                x_2d = np.array(temp_ffp['x_2d']) + station_x\n",
    "                y_2d = np.array(temp_ffp['y_2d']) + station_y\n",
    "                f_2d = f_2d*dx**2\n",
    "\n",
    "                #Calculate affine transform for given x_2d and y_2d\n",
    "                affine_transform = ff.find_transform(x_2d,y_2d)\n",
    "\n",
    "                #Create data file if not already created\n",
    "                if new_dat is None:\n",
    "                    #print(f_2d.shape)\n",
    "                    new_dat = rasterio.open(\n",
    "                        out_f,'w',driver='GTiff',dtype=rasterio.float64,\n",
    "                        count=n_hrs,height=f_2d.shape[0],width=f_2d.shape[1],\n",
    "                        transform=affine_transform, crs=out_proj.srs,\n",
    "                        nodata=0.00000000e+000\n",
    "                    )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f'Hour {hour} footprint failed, band {band} not written.')\n",
    "\n",
    "                temp_ffp = None\n",
    "\n",
    "                continue\n",
    "\n",
    "            #Mask out points that are below a % threshold (defaults to 90%)\n",
    "            f_2d = ff.mask_fp_cutoff(f_2d)\n",
    "\n",
    "            #Write the new band\n",
    "            new_dat.write(f_2d, indx+1)\n",
    "\n",
    "            #Update tags with metadata\n",
    "            tag_dict = {'hour':f'{hour*100:04}',\n",
    "                        'wind_dir':temp_line['wind_dir'].values,\n",
    "                        'total_footprint':np.nansum(f_2d)}\n",
    "\n",
    "            new_dat.update_tags(indx+1,**tag_dict)\n",
    "\n",
    "        #Close dataset if it exists\n",
    "        try:\n",
    "            new_dat.close()\n",
    "        except:\n",
    "            print(f'ERROR: could not write footprint for site: {station}:\\nto: {out_f}')\n",
    "            continue # skip to next day...\n",
    "\n",
    "        # for NLDAS data from pymetric\n",
    "        for hour in hours_zero_indexed:\n",
    "            #NLDAS version 2, primary forcing set (a), DOY must be 3 digit zero padded, HH 2-digit between 00-23, MM and DD also 2 digit\n",
    "            YYYY = date.year\n",
    "            DOY = date.timetuple().tm_yday\n",
    "            MM = date.month\n",
    "            DD = date.day\n",
    "            HH = hour\n",
    "\n",
    "            nldas_outf_path = nldas_out_dir / f'{YYYY}_{MM:02}_{DD:02}_{HH:02}.grb'\n",
    "            if nldas_outf_path.is_file():\n",
    "                print(f'{nldas_outf_path} already exists, not overwriting.')\n",
    "                pass\n",
    "                # do not overwrite!\n",
    "            else:\n",
    "                data_url = f'https://hydro1.gesdisc.eosdis.nasa.gov/data/NLDAS/NLDAS_FORA0125_H.002/{YYYY}/{DOY:03}/NLDAS_FORA0125_H.A{YYYY}{MM:02}{DD:02}.{HH:02}00.002.grb'\n",
    "                session = requests.Session()\n",
    "                r1 = session.request('get', data_url)\n",
    "                r = session.get(r1.url, stream=True, auth=(ed_user, ed_pass))\n",
    "\n",
    "                # write grib file temporarily\n",
    "                with open(nldas_outf_path, 'wb') as outf:\n",
    "                    for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
    "                        if chunk:  # filter out keep-alive new chunks\n",
    "                            outf.write(chunk)\n",
    "\n",
    "\n",
    "        # get hourly time series of ETr and ETo, save\n",
    "        for hour in hours_zero_indexed:\n",
    "            #NLDAS version 2, primary forcing set (a), DOY must be 3 digit zero padded, HH 2-digit between 00-23, MM and DD also 2 digit\n",
    "            YYYY = date.year\n",
    "            DOY = date.timetuple().tm_yday\n",
    "            MM = date.month\n",
    "            DD = date.day\n",
    "            HH = hour\n",
    "            # already ensured to exist above loop\n",
    "            nldas_outf_path = nldas_out_dir / f'{YYYY}_{MM:02}_{DD:02}_{HH:02}.grb'\n",
    "            # open grib and extract needed data at nearest gridcell, calc ETr/ETo anf append to time series\n",
    "            ds = xarray.open_dataset(nldas_outf_path,engine='pynio').sel(lat_110=latitude, lon_110=longitude, method='nearest')\n",
    "            # calculate hourly ea from specific humidity\n",
    "            pair = ds.get('PRES_110_SFC').data / 1000 # nldas air pres in Pa convert to kPa\n",
    "            sph = ds.get('SPF_H_110_HTGL').data # kg/kg\n",
    "            ea = refet.calcs._actual_vapor_pressure(q=sph, pair=pair) # ea in kPa\n",
    "            # calculate hourly wind\n",
    "            wind_u = ds.get('U_GRD_110_HTGL').data\n",
    "            wind_v = ds.get('V_GRD_110_HTGL').data\n",
    "            wind = np.sqrt(wind_u ** 2 + wind_v ** 2)\n",
    "            # get temp convert to C\n",
    "            temp = ds.get('TMP_110_HTGL').data - 273.15\n",
    "            # get rs\n",
    "            rs = ds.get('DSWRF_110_SFC').data\n",
    "            unit_dict = {'rs': 'w/m2'}\n",
    "            # create refet object for calculating\n",
    "\n",
    "            refet_obj = refet.Hourly(\n",
    "                tmean=temp, ea=ea, rs=rs, uz=wind,\n",
    "                zw=zm, elev=elevation, lat=latitude, lon=longitude,\n",
    "                doy=DOY, time=HH, method='asce', input_units=unit_dict) #HH must be int\n",
    "\n",
    "            # this one is saved under the site_ID subdir\n",
    "            nldas_ts_outf = out_dir/ f'nldas_ETr.csv'\n",
    "            # save/append time series of point data\n",
    "            dt = pd.datetime(YYYY,MM,DD,HH)\n",
    "            ETr_df = pd.DataFrame(columns=['ETr','ETo','ea','sph','wind','pair','temp','rs'])\n",
    "            ETr_df.loc[dt, 'ETr'] = refet_obj.etr()[0]\n",
    "            ETr_df.loc[dt, 'ETo'] = refet_obj.eto()[0]\n",
    "            ETr_df.loc[dt, 'ea'] = ea[0]\n",
    "            ETr_df.loc[dt, 'sph'] = sph\n",
    "            ETr_df.loc[dt, 'wind'] = wind\n",
    "            ETr_df.loc[dt, 'pair'] = pair\n",
    "            ETr_df.loc[dt, 'temp'] = temp\n",
    "            ETr_df.loc[dt, 'rs'] = rs\n",
    "            ETr_df.index.name = 'date'\n",
    "\n",
    "            # if first run save file with individual datetime (hour data) else open and overwrite hour\n",
    "            if not nldas_ts_outf.is_file():\n",
    "                ETr_df.round(4).to_csv(nldas_ts_outf)\n",
    "            else:\n",
    "                curr_df = pd.read_csv(nldas_ts_outf, index_col='date', parse_dates=True)\n",
    "                curr_df.loc[dt] = ETr_df.loc[dt]\n",
    "                curr_df.round(4).to_csv(nldas_ts_outf)    \n",
    "\n",
    "        # do hourly weighting - do not necessarily need to do this all in the same loop\n",
    "        src = rasterio.open(out_f)\n",
    "        # hourly fetch scalar sums\n",
    "        global_sum = np.zeros(shape=(n_hrs))\n",
    "        for hour in range(1,n_hrs+1):\n",
    "            arr = src.read(hour)\n",
    "            global_sum[hour-1] = arr.sum()\n",
    "        # normalized fetch rasters\n",
    "        normed_fetch_rasters = [] \n",
    "        for hour in range(1,n_hrs+1):\n",
    "            arr = src.read(hour)\n",
    "            tmp = arr / global_sum[hour-1]\n",
    "            if np.isnan(tmp).all():\n",
    "                tmp = np.zeros_like(tmp)\n",
    "            normed_fetch_rasters.append(tmp)\n",
    "        # get NLDAS ts calc fraction of daily ETo\n",
    "        nldas_df = pd.read_csv(nldas_ts_outf, index_col='date', parse_dates=True).sort_index()\n",
    "        ETo = nldas_df.loc[nldas_df.index.date == date, 'ETo']\n",
    "        min_max_normed_ETo = (ETo-min(ETo))/(max(ETo)-min(ETo)) # deal with negative ETo value proportions\n",
    "        # take out hours where footprint does not exist\n",
    "        i = 0\n",
    "        for e, s in zip(min_max_normed_ETo.values, global_sum):\n",
    "            if s == 0:\n",
    "                min_max_normed_ETo.iloc[i] = 0\n",
    "            i+=1\n",
    "        # after removing hours now calculate hourly proportions\n",
    "        nldas_df.loc[nldas_df.index.date == date, 'ETo_hr_props'] = min_max_normed_ETo / min_max_normed_ETo.sum()\n",
    "        # weight normed hourly fetch rasters by hourly ETo proportions\n",
    "        for i,hour in enumerate(hours_zero_indexed): # everything here is hours 0-23\n",
    "            normed_fetch_rasters[i] =\\\n",
    "                normed_fetch_rasters[i]*nldas_df.loc[\n",
    "                    (nldas_df.index.date == date) & (nldas_df.index.hour == hour), 'ETo_hr_props'\n",
    "            ].values[0]\n",
    "        # save hourly proportions to time series file\n",
    "        nldas_df.round(4).to_csv(nldas_ts_outf)\n",
    "\n",
    "        # Last calculation, sum the weighted hourly rasters to a single daily fetch raster\n",
    "        final_footprint = sum(normed_fetch_rasters)\n",
    "        assert np.isclose(final_footprint.sum(), 1), f'check 1 failed! {final_footprint.sum()}\\n{temp_line}'\n",
    "        # next check\n",
    "        for hour, raster in enumerate(normed_fetch_rasters):\n",
    "            assert np.isclose(\n",
    "                nldas_df.loc[\n",
    "                    (nldas_df.index.date == date) & (nldas_df.index.hour == hour+start_hr-1), 'ETo_hr_props'\n",
    "                ].values[0], raster.sum()\n",
    "            ), f'check 2 failed for hour {hour+start_hr-1}!!!'\n",
    "\n",
    "        # finally, write daily corrected raster with UTM zone reference \n",
    "        corr_raster_path = final_outf\n",
    "        out_raster = rasterio.open(\n",
    "            corr_raster_path,'w',driver='GTiff',dtype=rasterio.float64,\n",
    "            count=1,height=final_footprint.shape[0],width=final_footprint.shape[1],\n",
    "            transform=src.transform, crs=out_proj.srs, nodata=0.00000000e+000\n",
    "        )\n",
    "        out_raster.write(final_footprint,1)\n",
    "        out_raster.close()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = mp.Pool(processes=8)\n",
    "pool.map(runner,hourly_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
